# ğŸ¤Ÿ ASL Alphabet Translator# sign.Translate - ASL Alphabet Recognition



**American Sign Language to Text Translator** - Real-time hand gesture recognition using MediaPipe and TensorFlow.Real-time American Sign Language (ASL) alphabet recognition using TensorFlow.js and MediaPipe Hands.



## âœ¨ Features![Modern UI](https://img.shields.io/badge/UI-Modern%20Clean-8b5cf6)

![TensorFlow.js](https://img.shields.io/badge/TensorFlow.js-4.11.0-orange)

- ğŸ¥ **Real-time Detection** - Live camera feed with hand tracking![MediaPipe](https://img.shields.io/badge/MediaPipe-Hands-blue)

- ğŸ¤– **AI-Powered** - CNN model trained on 78,000+ ASL alphabet images

- ğŸ¨ **Clean Black & White UI** - Minimalist interface design## âœ¨ Features

- ğŸ—£ï¸ **Text-to-Speech** - Convert translated text to audio

- ğŸ“± **Mobile Support** - Responsive design works on all devices- ğŸ¯ **Real-time ASL A-Z detection** with ML model

- âœ‹ **Gesture Controls** - Pause/resume with hand gestures- ğŸ¨ **Modern clean UI** inspired by professional translators

- ğŸ“¹ **Auto-start camera** with instant detection

## ğŸš€ Quick Start- ğŸ‘† **Gesture controls** - Thumbs Up to pause, Open Palm to resume

- ğŸ—£ï¸ **Text-to-speech** for translations

### Prerequisites- ğŸ“‹ **Copy to clipboard** functionality

- ğŸ“– **Built-in ASL alphabet reference**

- Python 3.13+

- Modern web browser (Chrome/Firefox recommended)## ğŸš€ Quick Start

- Webcam

### Option 1: Run Without Training (Fallback Mode)

### Installation

```bash

```bashpython3 -m http.server 8000

# Clone repository```

git clone <your-repo-url>

cd project-signOpen: `http://localhost:8000`



# Create virtual environmentThe app will work with rule-based detection while showing "Fallback Mode".

python3 -m venv .venv

source .venv/bin/activate  # Linux/Mac### Option 2: Train Custom ML Model

# or

.venv\Scripts\activate  # Windows**1. Install Dependencies:**



# Install dependencies```bash

pip install flask flask-cors tensorflow opencv-python pillow numpy./setup.sh

```# or manually:

pip install opencv-python mediapipe tensorflow numpy scikit-learn tensorflowjs

### Running the Application```



**Terminal 1 - Start API Server:****2. Collect Training Data:**

```bash

python3 api_server.py```bash

```cd models

python3 collect_data.py

Wait for output:```

```

ğŸš€ ASL Alphabet Recognition API- Collect 150 samples per letter (A-Z)

Running on http://localhost:5000- Total: 3,900 samples

```- Takes ~1-2 hours



**Terminal 2 - Start Web Server:****3. Train Model:**

```bash

python3 -m http.server 8000```bash

```python3 train_model.py

```

**Browser:**

Open http://localhost:8000- Training time: ~15-30 minutes

- Output: `asl_alphabet_model/` folder

## ğŸ¯ How to Use

**4. Refresh Browser:**

### Making Gestures

Model will auto-load and status will show "Loaded âœ“" (green).

1. **Allow camera access** when prompted

2. **Position your hand** clearly in front of the camera## ğŸ“– Documentation

3. **Hold gesture** for 2-3 seconds

4. **Watch translation** appear in the right panel- **ASL_ALPHABET_GUIDE.md** - Complete training guide

- **ASL_REFERENCE.md** - ASL alphabet quick reference

### Gesture Controls

## ğŸ® How to Use

- **ğŸ‘ Thumbs Up** = Pause translation

- **âœ‹ Open Palm** = Resume translation1. **Open the app** - Camera starts automatically

2. **Make ASL gestures** - Letters A-Z are detected

### Action Buttons3. **Translation appears** - Text builds in real-time

4. **Use controls:**

- **ğŸ”Š** - Text-to-speech (read translated text)   - ğŸ”Š Speak - Text-to-speech

- **ğŸ—‘ï¸** - Clear all text   - ğŸ—‘ï¸ Clear - Clear translation

- **ğŸ“‹** - Copy text to clipboard   - ğŸ“‹ Copy - Copy to clipboard

   - ğŸ”¤ Alphabet - View ASL reference

## ğŸ“ Project Structure

### Gesture Controls:

```- ğŸ‘ **Thumbs Up** (hold 1s) â†’ **PAUSE** detection

project-sign/- âœ‹ **Open Palm** (hold 1s) â†’ **RESUME** detection

â”œâ”€â”€ index.html              # Main web application

â”œâ”€â”€ alphabet.html           # ASL alphabet reference guide## ğŸ—ï¸ Tech Stack

â”œâ”€â”€ styles.css              # Black & white theme styles

â”œâ”€â”€ script.js               # Frontend logic- **Frontend:** Vanilla JavaScript, CSS3

â”œâ”€â”€ api_client.js          # API communication- **ML:** TensorFlow.js (LayersModel)

â”œâ”€â”€ api_server.py          # Backend inference server- **Hand Tracking:** MediaPipe Hands

â”œâ”€â”€ models/- **Training:** Python, TensorFlow, Keras

â”‚   â”œâ”€â”€ asl_alphabet_model_quick.h5    # Trained model

â”‚   â””â”€â”€ train_asl_model.py             # Training script## ğŸ“ Project Structure

â””â”€â”€ .venv/                             # Python environment

``````

project-sign/

## ğŸ”§ Configurationâ”œâ”€â”€ index.html                  # Main app (clean UI)

â”œâ”€â”€ styles.css                  # Modern dark theme

### API Serverâ”œâ”€â”€ script.js                   # Core logic + ML

â”œâ”€â”€ setup.sh                    # Quick setup script

Edit `api_server.py`:â”œâ”€â”€ models/

```pythonâ”‚   â”œâ”€â”€ collect_data.py         # Data collection

# Change portâ”‚   â”œâ”€â”€ train_model.py          # Model training

app.run(host='0.0.0.0', port=5000)â”‚   â”œâ”€â”€ check_dependencies.py   # Dependency checker

â”‚   â””â”€â”€ asl_alphabet_model/     # Trained model (after training)

# Change modelâ””â”€â”€ README.md                   # This file

MODEL_PATH = 'models/your_model.h5'```

```

## ğŸ¯ Model Performance

### Frontend

With proper training:

Edit `api_client.js`:- **Accuracy:** 85-95%

```javascript- **Input:** 63 features (21 landmarks Ã— 3 coords)

// Change API URL- **Output:** 26 classes (A-Z)

const API_URL = 'http://localhost:5000';- **Architecture:** 128â†’64â†’32â†’26 (Dense layers)



// Change prediction interval## ğŸ› Troubleshooting

const CAPTURE_INTERVAL = 1000; // milliseconds

```**Model Status shows "Fallback Mode":**

- Model not trained yet

## ğŸ¨ Color Theme- Train following ASL_ALPHABET_GUIDE.md



This application uses a **strict black and white palette**:**Low accuracy:**

- Collect more data (200+ samples/letter)

- Background: `#000000` (black)- Ensure good lighting

- Text: `#FFFFFF` (white)- Hold gestures steady

- Borders: `#FFFFFF33` (white with transparency)

- No other colors are used**Camera not working:**

- Allow camera permission

## ğŸ§ª Model Training- Use HTTPS or localhost

- Check no other app using camera

To retrain the model with your own dataset:

## ğŸ“± Responsive Design

```bash

cd models- âœ… Desktop: Two-panel layout (video | translation)

- âœ… Tablet: Stacked layout

# Place your dataset in models/asl_alphabet_train/- âœ… Mobile: Optimized controls



# Train model## ğŸ¨ UI Design

python3 train_asl_model.py

```Inspired by modern translation apps with:

- Clean two-panel layout

**Dataset Structure:**

```- Minimal distractions

asl_alphabet_train/- Professional appearance

  asl_alphabet_train/

    A/ (3000 images)## ğŸ”® Future Enhancements

    B/ (3000 images)

    ...- [ ] Words & phrases detection

    Z/ (3000 images)- [ ] Multiple sign language support (BSL, etc.)

```- [ ] History/bookmarks

- [ ] Voice commands

## ğŸ“Š Model Specifications- [ ] Mobile app version



- **Architecture**: CNN (Conv2D layers + Dense layers)## ğŸ“œ License

- **Input**: 50x50 grayscale images

- **Output**: 26 classes (A-Z)MIT License - feel free to use for learning and projects!

- **Training**: 20 epochs, Adam optimizer

- **Expected Accuracy**: 85-95%## ğŸ™ Credits



## ğŸ› ï¸ Troubleshooting- TensorFlow.js team

- MediaPipe team

### API Server Not Connecting- ASL community



**Solution:**---

```bash

# Check if server is running**Made with â¤ï¸ for accessibility**

curl http://localhost:5000/health

# Restart server
pkill -f api_server
python3 api_server.py
```

### Camera Not Working

1. Refresh browser (F5)
2. Check camera permissions in browser settings
3. Ensure no other app is using camera

### Low Accuracy

1. Improve lighting conditions
2. Position hand clearly in frame
3. Hold gesture steady for 2-3 seconds
4. Retrain model with more data

## ğŸ“ API Endpoints

### `GET /health`
Health check endpoint

**Response:**
```json
{
  "status": "ok",
  "model": "loaded"
}
```

### `POST /predict`
Predict ASL letter from image

**Request:**
```json
{
  "image": "data:image/jpeg;base64,..."
}
```

**Response:**
```json
{
  "prediction": "A",
  "confidence": 0.95,
  "top3": [
    {"label": "A", "confidence": 0.95},
    {"label": "S", "confidence": 0.03},
    {"label": "M", "confidence": 0.01}
  ]
}
```

### `GET /labels`
Get all supported labels

**Response:**
```json
{
  "labels": ["A", "B", "C", ..., "Z"]
}
```

## ğŸŒ Browser Support

- âœ… Chrome 90+
- âœ… Firefox 88+
- âœ… Safari 14+
- âœ… Edge 90+

## ğŸ“„ License

MIT License - feel free to use and modify

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create feature branch
3. Commit changes
4. Push to branch
5. Open pull request

## ğŸ™ Credits

- **MediaPipe** - Hand tracking
- **TensorFlow** - Machine learning
- **ASL Dataset** - Kaggle ASL Alphabet dataset

---

Made with â¤ï¸ for the deaf and hard-of-hearing community
