{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c6e4af8",
   "metadata": {},
   "source": [
    "# ü§ü ASL Alphabet Recognition - Model Training\n",
    "\n",
    "Training CNN model untuk mengenali 25 huruf ASL (A-Z tanpa J)\n",
    "\n",
    "**Dataset:** Kaggle ASL Alphabet  \n",
    "**Input:** 28x28 grayscale images  \n",
    "**Output:** 25 classes (A-Z without J)  \n",
    "**Goal:** Model yang robust untuk webcam real-world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9179a",
   "metadata": {},
   "source": [
    "## üì¶ Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba62ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tensorflow pillow matplotlib scikit-learn opencv-python\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585215c5",
   "metadata": {},
   "source": [
    "## üì• Download Dataset\n",
    "\n",
    "**Option 1:** Download dari Kaggle (Recommended)\n",
    "\n",
    "1. Buat Kaggle API token:\n",
    "   - Go to https://www.kaggle.com/settings\n",
    "   - Create New API Token\n",
    "   - Upload `kaggle.json` di cell berikutnya\n",
    "\n",
    "**Option 2:** Upload dataset manual\n",
    "- Download: https://www.kaggle.com/grassknoted/asl-alphabet\n",
    "- Upload zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cb58cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 1: Kaggle API (RECOMMENDED)\n",
    "\n",
    "# Upload kaggle.json\n",
    "print(\"Upload your kaggle.json file...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Setup Kaggle\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# Download dataset\n",
    "!kaggle datasets download -d grassknoted/asl-alphabet\n",
    "\n",
    "# Extract\n",
    "!unzip -q asl-alphabet.zip -d asl_data\n",
    "print(\"‚úì Dataset downloaded and extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTION 2: Manual Upload (jika Option 1 tidak bisa)\n",
    "\n",
    "# Uncomment jika pakai manual upload:\n",
    "# print(\"Upload asl-alphabet.zip file...\")\n",
    "# uploaded = files.upload()\n",
    "# !unzip -q asl-alphabet.zip -d asl_data\n",
    "# print(\"‚úì Dataset extracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6098b690",
   "metadata": {},
   "source": [
    "## üìÇ Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac6109c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset structure\n",
    "data_dir = 'asl_data/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "# List all classes\n",
    "classes = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])\n",
    "print(f\"Total classes: {len(classes)}\")\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Remove 'J' and other non-letter classes\n",
    "valid_letters = [c for c in classes if c in 'ABCDEFGHIKLMNOPQRSTUVWXYZ']  # No J\n",
    "print(f\"\\nValid ASL letters (without J): {len(valid_letters)}\")\n",
    "print(f\"Letters: {valid_letters}\")\n",
    "\n",
    "# Count images per class\n",
    "for letter in valid_letters[:5]:  # Show first 5\n",
    "    letter_dir = os.path.join(data_dir, letter)\n",
    "    num_images = len(os.listdir(letter_dir))\n",
    "    print(f\"{letter}: {num_images} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7557bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample images\n",
    "def show_samples(num_samples=5):\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(15, 15))\n",
    "    fig.suptitle('Sample ASL Letters', fontsize=16)\n",
    "    \n",
    "    for idx, letter in enumerate(valid_letters[:25]):\n",
    "        letter_dir = os.path.join(data_dir, letter)\n",
    "        images = os.listdir(letter_dir)[:1]  # Take first image\n",
    "        \n",
    "        img_path = os.path.join(letter_dir, images[0])\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        row = idx // 5\n",
    "        col = idx % 5\n",
    "        axes[row, col].imshow(img)\n",
    "        axes[row, col].set_title(f\"{letter}\", fontsize=14, fontweight='bold')\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b12fceb",
   "metadata": {},
   "source": [
    "## üîÑ Data Preprocessing with Hand Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c4339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "IMG_SIZE = 28\n",
    "LABELS = valid_letters\n",
    "NUM_CLASSES = len(LABELS)\n",
    "\n",
    "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Labels: {LABELS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a79e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_hand_region(image):\n",
    "    \"\"\"\n",
    "    Crop image to hand region only (remove background)\n",
    "    Using simple thresholding and bounding box detection\n",
    "    \"\"\"\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # Threshold to separate hand from background\n",
    "    # Assuming hand is brighter than background\n",
    "    _, thresh = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    if len(contours) == 0:\n",
    "        return image  # Return original if no contours found\n",
    "    \n",
    "    # Get largest contour (should be hand)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "    \n",
    "    # Add margin (10%)\n",
    "    margin = int(max(w, h) * 0.1)\n",
    "    x = max(0, x - margin)\n",
    "    y = max(0, y - margin)\n",
    "    w = min(image.width - x, w + 2*margin)\n",
    "    h = min(image.height - y, h + 2*margin)\n",
    "    \n",
    "    # Crop\n",
    "    cropped = image.crop((x, y, x+w, y+h))\n",
    "    return cropped\n",
    "\n",
    "def preprocess_image(img_path, use_crop=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess image:\n",
    "    1. Load image\n",
    "    2. Crop hand region (optional)\n",
    "    3. Convert to grayscale\n",
    "    4. Resize to 28x28\n",
    "    5. Normalize to [0, 1]\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Crop hand region\n",
    "    if use_crop:\n",
    "        try:\n",
    "            img = crop_hand_region(img)\n",
    "        except:\n",
    "            pass  # Use original if cropping fails\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    img = img.convert('L')\n",
    "    \n",
    "    # Resize\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Convert to array and normalize\n",
    "    img_array = np.array(img, dtype=np.float32) / 255.0\n",
    "    \n",
    "    return img_array\n",
    "\n",
    "# Test preprocessing\n",
    "test_letter = LABELS[0]\n",
    "test_dir = os.path.join(data_dir, test_letter)\n",
    "test_img = os.path.join(test_dir, os.listdir(test_dir)[0])\n",
    "\n",
    "# Original vs Cropped\n",
    "img_original = preprocess_image(test_img, use_crop=False)\n",
    "img_cropped = preprocess_image(test_img, use_crop=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(img_original, cmap='gray')\n",
    "axes[0].set_title('Original (No Crop)')\n",
    "axes[0].axis('off')\n",
    "axes[1].imshow(img_cropped, cmap='gray')\n",
    "axes[1].set_title('Hand Cropped')\n",
    "axes[1].axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Preprocessing test completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc335a30",
   "metadata": {},
   "source": [
    "## üìä Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all images (this may take a few minutes)\n",
    "MAX_IMAGES_PER_CLASS = 3000  # Limit untuk speed (total 75,000 images)\n",
    "USE_HAND_CROP = True  # Set True untuk hand cropping\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "print(\"Loading images...\")\n",
    "for label_idx, letter in enumerate(LABELS):\n",
    "    letter_dir = os.path.join(data_dir, letter)\n",
    "    images = os.listdir(letter_dir)[:MAX_IMAGES_PER_CLASS]\n",
    "    \n",
    "    print(f\"Loading {letter}: {len(images)} images...\", end=\" \")\n",
    "    \n",
    "    for img_name in images:\n",
    "        img_path = os.path.join(letter_dir, img_name)\n",
    "        try:\n",
    "            img_array = preprocess_image(img_path, use_crop=USE_HAND_CROP)\n",
    "            X.append(img_array)\n",
    "            y.append(label_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(\"‚úì\")\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Reshape untuk Keras (add channel dimension)\n",
    "X = X.reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "\n",
    "print(f\"\\n‚úì Dataset loaded!\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"Total images: {len(X)}\")\n",
    "print(f\"Images per class: ~{len(X) // NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3102e2b9",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befc01a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=y  # Ensure balanced split\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"\\nClass distribution (training):\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for label_idx, count in zip(unique[:5], counts[:5]):\n",
    "    print(f\"  {LABELS[label_idx]}: {count} images\")\n",
    "print(f\"  ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1923a",
   "metadata": {},
   "source": [
    "## üß† Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14191244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Create CNN model with:\n",
    "    - 3 Convolutional blocks\n",
    "    - Batch Normalization\n",
    "    - Dropout for regularization\n",
    "    - Dense layers\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        # Input layer\n",
    "        layers.Input(shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "        \n",
    "        # Conv Block 1\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 2\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Conv Block 3\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e3e6e",
   "metadata": {},
   "source": [
    "## üéì Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation untuk meningkatkan robustness\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomRotation(0.1),  # ¬±10 degrees\n",
    "    layers.RandomZoom(0.1),  # ¬±10% zoom\n",
    "    layers.RandomTranslation(0.1, 0.1),  # ¬±10% shift\n",
    "])\n",
    "\n",
    "# Visualize augmentation\n",
    "sample_img = X_train[0:1]\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('Data Augmentation Examples', fontsize=14)\n",
    "\n",
    "for i in range(8):\n",
    "    augmented = data_augmentation(sample_img, training=True)\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    axes[row, col].imshow(augmented[0].numpy().squeeze(), cmap='gray')\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Augmentation configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e854ca",
   "metadata": {},
   "source": [
    "## üöÄ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e91a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    # Early stopping\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Learning rate reduction\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "print(f\"Starting training...\")\n",
    "print(f\"Epochs: {EPOCHS}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(\"\\nTraining started...\\n\")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24139d3a",
   "metadata": {},
   "source": [
    "## üìà Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14bee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Training')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
    "axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Training')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation')\n",
    "axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "print(f\"Final Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b66664",
   "metadata": {},
   "source": [
    "## üß™ Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66603f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = keras.models.load_model('best_model.h5')\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_val, verbose=0)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_val, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=LABELS, yticklabels=LABELS)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred_classes, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6da8e1",
   "metadata": {},
   "source": [
    "## üéØ Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7058a523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "num_samples = 16\n",
    "indices = np.random.choice(len(X_val), num_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n",
    "fig.suptitle('Sample Predictions', fontsize=16, fontweight='bold')\n",
    "\n",
    "for i, idx in enumerate(indices):\n",
    "    img = X_val[idx]\n",
    "    true_label = LABELS[y_val[idx]]\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(img.reshape(1, IMG_SIZE, IMG_SIZE, 1), verbose=0)[0]\n",
    "    pred_label = LABELS[np.argmax(pred)]\n",
    "    confidence = np.max(pred) * 100\n",
    "    \n",
    "    # Plot\n",
    "    row = i // 4\n",
    "    col = i % 4\n",
    "    axes[row, col].imshow(img.squeeze(), cmap='gray')\n",
    "    \n",
    "    color = 'green' if pred_label == true_label else 'red'\n",
    "    axes[row, col].set_title(\n",
    "        f\"True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)\",\n",
    "        fontsize=12, color=color, fontweight='bold'\n",
    "    )\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f43bfef",
   "metadata": {},
   "source": [
    "## üíæ Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save('asl_model_new.h5')\n",
    "print(\"‚úì Model saved as 'asl_model_new.h5'\")\n",
    "\n",
    "# Download model\n",
    "files.download('asl_model_new.h5')\n",
    "print(\"‚úì Model downloaded!\")\n",
    "\n",
    "# Model info\n",
    "import os\n",
    "file_size = os.path.getsize('asl_model_new.h5') / (1024 * 1024)\n",
    "print(f\"\\nModel size: {file_size:.2f} MB\")\n",
    "print(f\"Input shape: (28, 28, 1)\")\n",
    "print(f\"Output classes: {NUM_CLASSES}\")\n",
    "print(f\"Validation accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca1178",
   "metadata": {},
   "source": [
    "## üß™ Test with Custom Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1800e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload test image\n",
    "print(\"Upload an ASL hand sign image to test...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename in uploaded.keys():\n",
    "    print(f\"\\nTesting: {filename}\")\n",
    "    \n",
    "    # Preprocess\n",
    "    img = preprocess_image(filename, use_crop=USE_HAND_CROP)\n",
    "    img_input = img.reshape(1, IMG_SIZE, IMG_SIZE, 1)\n",
    "    \n",
    "    # Predict\n",
    "    pred = model.predict(img_input, verbose=0)[0]\n",
    "    top_3_idx = np.argsort(pred)[-3:][::-1]\n",
    "    \n",
    "    # Display\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(img, cmap='gray')\n",
    "    axes[0].set_title(f\"Input Image (28x28)\", fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Top 3 predictions\n",
    "    labels_plot = [LABELS[i] for i in top_3_idx]\n",
    "    probs_plot = [pred[i] * 100 for i in top_3_idx]\n",
    "    \n",
    "    axes[1].barh(labels_plot, probs_plot, color=['green', 'orange', 'red'])\n",
    "    axes[1].set_xlabel('Confidence (%)', fontsize=12)\n",
    "    axes[1].set_title('Top 3 Predictions', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlim(0, 100)\n",
    "    \n",
    "    for i, (label, prob) in enumerate(zip(labels_plot, probs_plot)):\n",
    "        axes[1].text(prob + 2, i, f\"{prob:.1f}%\", va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nPrediction: {LABELS[top_3_idx[0]]}\")\n",
    "    print(f\"Confidence: {pred[top_3_idx[0]]*100:.2f}%\")\n",
    "    print(f\"\\nTop 3:\")\n",
    "    for i in top_3_idx:\n",
    "        print(f\"  {LABELS[i]}: {pred[i]*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5303215c",
   "metadata": {},
   "source": [
    "## üìù Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bcdb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: Kaggle ASL Alphabet\")\n",
    "print(f\"Hand Cropping: {USE_HAND_CROP}\")\n",
    "print(f\"Total images: {len(X)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"\")\n",
    "print(f\"Model architecture: CNN with BatchNorm + Dropout\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "print(f\"\")\n",
    "print(f\"Training epochs: {len(history.history['accuracy'])}\")\n",
    "print(f\"Final training accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Final validation accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(f\"\")\n",
    "print(f\"Model saved: asl_model_new.h5 ({file_size:.2f} MB)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"‚úì Training completed successfully!\")\n",
    "print(\"‚úì Model ready for deployment!\")\n",
    "print(\"\")\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"1. Download 'asl_model_new.h5'\")\n",
    "print(\"2. Replace old model in your project\")\n",
    "print(\"3. Test with real webcam\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
